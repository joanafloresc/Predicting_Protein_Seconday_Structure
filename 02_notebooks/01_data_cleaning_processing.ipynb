{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b2776a",
   "metadata": {},
   "source": [
    "# Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc1acb",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87eafe",
   "metadata": {},
   "source": [
    "Importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a71a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06875163",
   "metadata": {},
   "source": [
    "Reading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3731885",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('../params.yml','r') as file:\n",
    "        config = yaml.safe_load(file)  \n",
    "except Exception as e:\n",
    "    print('Error reading the config file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e54439",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6db601",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv(config['raw_data_1']).iloc[:,1:]\n",
    "data_2 = pd.read_csv(config['raw_data_2']).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612426e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0376035",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_1,data_2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(keep='first', inplace=True, ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('../01_data/data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7c753",
   "metadata": {},
   "source": [
    "#### Remarks:\n",
    "pdb_id : protein reference\\\n",
    "seq: Aminoacid sequende\\\n",
    "sst3: secondary structure labeled with 3 categories:\\\n",
    "&emsp; H - Helix\\\n",
    "&emsp; E - B-strand \\\n",
    "&emsp; C - Irregular elements\\\n",
    " \\\n",
    "sst8: secondary structure labeled with 8 categories:\\\n",
    "&emsp; H - α-helix\\\n",
    "&emsp; G - 3-helix\\\n",
    "&emsp; I - π-helix\\\n",
    " \\\n",
    "&emsp; E - β-strand\\\n",
    "&emsp; B - β-bridge\\\n",
    " \\\n",
    "&emsp; C - Loops and irregular elements (corresponding to the blank characters output by DSSP)\\\n",
    "&emsp; T - Turn\\\n",
    "&emsp; S - Bend\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafda529",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6e9ac",
   "metadata": {},
   "source": [
    "Get a dataset of each aminoacid, the 2 aminoacids before and 2 after, and the secondary structure (3-labels and 8-labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_columns(data):\n",
    "    '''function to break proteins into 20-characters-length sequences'''\n",
    "\n",
    "    df1 = pd.DataFrame()\n",
    "    for index in range(0,len(data['seq'])):\n",
    "        df2 = pd.DataFrame()\n",
    "        for n in range(2,len(data['seq'][index])-2):\n",
    "            parts = [ data['seq'][index][n-2], data['seq'][index][n-1], data['seq'][index][n],\n",
    "                     data['seq'][index][n+1], data['seq'][index][n+2], data['sst3'][index][n], data['sst8'][index][n]]\n",
    "            df1 = pd.concat([df1, pd.DataFrame(parts).T], axis=0)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "\n",
    "    df1 = pd.concat([df1,df2], axis=0)\n",
    "\n",
    "    df1.columns = ['AA-2','AA-1','AA','AA+1','AA+2','y3','y8']\n",
    "                \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe198dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data = split_columns(data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96131a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data = pd.read_csv(config['new_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data.shape)\n",
    "new_data.drop_duplicates(keep='first', inplace=True, ignore_index=True)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('../01_data/new_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e10ca0",
   "metadata": {},
   "source": [
    "The number of rows is too big so I am operating a downsizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = np.linspace(1, 3118982, 3068983, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "down_data = new_data.drop(labels=skip, axis=0)\n",
    "down_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493cfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = down_data.iloc[:,0:5]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = down_data.iloc[:,5:]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d59de8",
   "metadata": {},
   "source": [
    "Encoding AA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encoding_x(data):\n",
    "    \n",
    "    #get dataset with unique values of AA\n",
    "    aa_types=[]\n",
    "    for col in data.columns:\n",
    "        for a in data[col]:\n",
    "            if a not in aa_types:\n",
    "                aa_types.append(a)\n",
    "\n",
    "    aa_types.sort()\n",
    "    aa_df = pd.DataFrame(aa_types)\n",
    "    aa_df.columns = ['AA']\n",
    "    \n",
    "    #get a encoder-code for each AA\n",
    "    \n",
    "    encoder= OneHotEncoder().fit(aa_df)\n",
    "    aa_enc = encoder.transform(aa_df).toarray()\n",
    "    data_c = data.copy()\n",
    "    \n",
    "    #converse every AA in dataset by its own encoder\n",
    "    \n",
    "    x = pd.DataFrame(index=range(0,len(data))) #new dataset\n",
    "\n",
    "    for col in data.columns:\n",
    "        list_ = []\n",
    "        for row in data[col]:\n",
    "            index = int(aa_df[aa_df['AA']==row].index.values)\n",
    "            row = aa_enc[index]\n",
    "            list_.append(row)\n",
    "        x[col] = list_\n",
    "        \n",
    "    #sum AA-encoders per sequence     \n",
    "    x_sum = x.sum(axis=1)\n",
    "    \n",
    "    #converting np.array into pd.DataFrame\n",
    "    all_df= pd.DataFrame()\n",
    "    for n in range(0,len(x_sum)):\n",
    "        all_df = pd.concat([all_df, pd.DataFrame(x_sum[n]).T], axis=0, ignore_index=True)\n",
    "        \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_enc = encoding_x(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = pd.concat([x_enc, clean_data.iloc[:,-2:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_enc = encoding_x(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e41919",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = pd.concat([x_enc, clean_data.iloc[:,-2:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523827f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xy.shape)\n",
    "xy.drop_duplicates(keep='first', inplace=True, ignore_index=True)\n",
    "print(xy.shape)\n",
    "xy.to_csv('../01_data/xy.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projw9",
   "language": "python",
   "name": "projw9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
